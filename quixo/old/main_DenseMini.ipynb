{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimax with alpha-beta pruning and state represented as integers for transposition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from game import Game, Move, Player\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "border = []\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i == 0 or i == 4 or j == 0 or j == 4:\n",
    "            border.append((i, j))\n",
    "BORDER = (list(set(border)))\n",
    "print(len(BORDER))\n",
    "\n",
    "def tile_to_moves(tile):\n",
    "    possible_moves = [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]\n",
    "        \n",
    "    if tile[0] == 0: possible_moves.remove(Move.LEFT)\n",
    "    if tile[0] == 4: possible_moves.remove(Move.RIGHT)\n",
    "    if tile[1] == 0: possible_moves.remove(Move.TOP)\n",
    "    if tile[1] == 4: possible_moves.remove(Move.BOTTOM)\n",
    "\n",
    "    return possible_moves\n",
    "\n",
    "tile_moves = {tile: tile_to_moves(tile) for tile in BORDER}\n",
    "\n",
    "ALL_MOVES = []\n",
    "for tile in BORDER:\n",
    "    possible_moves = tile_moves[tile]\n",
    "    for move in possible_moves: ALL_MOVES.append((tile, move))\n",
    "N_ALL = len(ALL_MOVES)\n",
    "print(N_ALL)\n",
    "\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "\n",
    "        from_pos = random.choice(BORDER)\n",
    "        while game.get_board()[from_pos[1], from_pos[0]] == 1 - game.current_player_idx: from_pos = random.choice(BORDER)\n",
    "\n",
    "        possible_moves = tile_moves[from_pos]\n",
    "        \n",
    "        move = random.choice(possible_moves)\n",
    "\n",
    "        return from_pos, move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## state can be represented as a number if 0,1 are considered as bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board:\n",
      "[[ 0  0 -1 -1  1]\n",
      " [ 0  0  1  0 -1]\n",
      " [ 1  0  0  0 -1]\n",
      " [ 0  0  0  0  0]\n",
      " [-1 -1  1 -1  0]]\n",
      "\n",
      "State:\n",
      "212240977510404\n",
      "\n",
      "Board:\n",
      "[[ 0  0 -1 -1  1]\n",
      " [ 0  0  1  0 -1]\n",
      " [ 1  0  0  0 -1]\n",
      " [ 0  0  0  0  0]\n",
      " [-1 -1  1 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def state_to_board(state):\n",
    "    binary_string = format(state, '050b')\n",
    "    binary_array = np.array(list(map(int, binary_string))).reshape(2, 5, 5)\n",
    "\n",
    "    board = np.zeros((5, 5), dtype=int)\n",
    "    board[binary_array[0] == 1] = -1\n",
    "    board[binary_array[1] == 1] = 1\n",
    "\n",
    "    return board\n",
    "\n",
    "def board_to_state(board):\n",
    "    binary_array = np.zeros((2, 5, 5), dtype=int)\n",
    "    \n",
    "    binary_array[0][board == -1] = 1\n",
    "    binary_array[1][board == 1] = 1\n",
    "\n",
    "    binary_string = ''.join(map(str, binary_array.flatten()))\n",
    "    return int(binary_string, 2)\n",
    "\n",
    "\n",
    "\n",
    "rand_board = np.random.choice([-1, 0, 1], size=(5, 5), replace=True)\n",
    "print('Board:')\n",
    "print(rand_board)\n",
    "\n",
    "rand_state = board_to_state(rand_board)\n",
    "rand_board = state_to_board(rand_state)\n",
    "\n",
    "print('\\nState:')\n",
    "print(rand_state)\n",
    "print('\\nBoard:')\n",
    "print(state_to_board(rand_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimeCounter:\n",
    "    def __init__(self):\n",
    "        self.tot_time = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def add_t(self, t):\n",
    "        self.tot_time += t\n",
    "        self.count += 1\n",
    "\n",
    "    def get(self): return self.tot_time, self.tot_time / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy_Game(object):\n",
    "    def __init__(self) -> None:\n",
    "        self._board = np.ones((5, 5), dtype=np.uint8) * -1\n",
    "        self.current_player_idx = 0\n",
    "\n",
    "    def get_board(self): return deepcopy(self._board)\n",
    "\n",
    "    def single_move(self, board, from_pos, move, player_id):\n",
    "        self._board = deepcopy(board)\n",
    "        self.current_player_idx = player_id\n",
    "        ok = self.__move(from_pos, move, player_id)\n",
    "        return deepcopy(self._board), ok\n",
    "    \n",
    "    def do_move(self, from_pos, move, player_id):\n",
    "        return self.__move(from_pos, move, player_id)\n",
    "\n",
    "    def check_winner(self) -> int:\n",
    "        for x in range(self._board.shape[0]):\n",
    "            if self._board[x, 0] != -1 and all(self._board[x, :] == self._board[x, 0]): return self._board[x, 0]\n",
    "        for y in range(self._board.shape[1]):\n",
    "            if self._board[0, y] != -1 and all(self._board[:, y] == self._board[0, y]): return self._board[0, y]\n",
    "        if self._board[0, 0] != -1 and all([self._board[x, x] for x in range(self._board.shape[0])] == self._board[0, 0]): return self._board[0, 0]\n",
    "        if self._board[0, -1] != -1 and all([self._board[x, -(x + 1)] for x in range(self._board.shape[0])] == self._board[0, -1]): return self._board[0, -1]\n",
    "        return -1\n",
    "\n",
    "    def __move(self, from_pos: tuple[int, int], slide: Move, player_id: int) -> bool:\n",
    "        if player_id > 2: return False\n",
    "        prev_value = deepcopy(self._board[(from_pos[1], from_pos[0])])\n",
    "        acceptable = self.__take((from_pos[1], from_pos[0]), player_id)\n",
    "        if acceptable:\n",
    "            acceptable = self.__slide((from_pos[1], from_pos[0]), slide)\n",
    "            if not acceptable: self._board[(from_pos[1], from_pos[0])] = deepcopy(prev_value)\n",
    "            if acceptable: self.current_player_idx = 1 - self.current_player_idx\n",
    "        return acceptable\n",
    "\n",
    "    def __take(self, from_pos: tuple[int, int], player_id: int) -> bool:\n",
    "        acceptable: bool = ((from_pos[0] == 0 and from_pos[1] < 5) or (from_pos[0] == 4 and from_pos[1] < 5) or (from_pos[1] == 0 and from_pos[0] < 5) or (from_pos[1] == 4 and from_pos[0] < 5)) and (self._board[from_pos] < 0 or self._board[from_pos] == player_id)\n",
    "        if acceptable: self._board[from_pos] = player_id\n",
    "        return acceptable\n",
    "\n",
    "    def __slide(self, from_pos: tuple[int, int], slide: Move) -> bool:\n",
    "        SIDES = [(0, 0), (0, 4), (4, 0), (4, 4)]\n",
    "        if from_pos not in SIDES:\n",
    "            acceptable_top: bool = from_pos[0] == 0 and (slide == Move.BOTTOM or slide == Move.LEFT or slide == Move.RIGHT)\n",
    "            acceptable_bottom: bool = from_pos[0] == 4 and (slide == Move.TOP or slide == Move.LEFT or slide == Move.RIGHT)\n",
    "            acceptable_left: bool = from_pos[1] == 0 and (slide == Move.BOTTOM or slide == Move.TOP or slide == Move.RIGHT)\n",
    "            acceptable_right: bool = from_pos[1] == 4 and (slide == Move.BOTTOM or slide == Move.TOP or slide == Move.LEFT)\n",
    "        else:\n",
    "            acceptable_top: bool = from_pos == (0, 0) and (slide == Move.BOTTOM or slide == Move.RIGHT)\n",
    "            acceptable_left: bool = from_pos == (4, 0) and (slide == Move.TOP or slide == Move.RIGHT)\n",
    "            acceptable_right: bool = from_pos == (0, 4) and (slide == Move.BOTTOM or slide == Move.LEFT)\n",
    "            acceptable_bottom: bool = from_pos == (4, 4) and (slide == Move.TOP or slide == Move.LEFT)\n",
    "        acceptable: bool = acceptable_top or acceptable_bottom or acceptable_left or acceptable_right\n",
    "        if acceptable:\n",
    "            piece = self._board[from_pos]\n",
    "            if slide == Move.LEFT:\n",
    "                for i in range(from_pos[1], 0, -1): self._board[(from_pos[0], i)] = self._board[(from_pos[0], i - 1)]\n",
    "                self._board[(from_pos[0], 0)] = piece\n",
    "            elif slide == Move.RIGHT:\n",
    "                for i in range(from_pos[1], self._board.shape[1] - 1, 1): self._board[(from_pos[0], i)] = self._board[(from_pos[0], i + 1)]\n",
    "                self._board[(from_pos[0], self._board.shape[1] - 1)] = piece\n",
    "            elif slide == Move.TOP:\n",
    "                for i in range(from_pos[0], 0, -1): self._board[(i, from_pos[1])] = self._board[(i - 1, from_pos[1])]\n",
    "                self._board[(0, from_pos[1])] = piece\n",
    "            elif slide == Move.BOTTOM:\n",
    "                for i in range(from_pos[0], self._board.shape[0] - 1, 1): self._board[(i, from_pos[1])] = self._board[(i + 1, from_pos[1])]\n",
    "                self._board[(self._board.shape[0] - 1, from_pos[1])] = piece\n",
    "        return acceptable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 0), <Move.BOTTOM: 1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minimax(game: \"Dummy_Game\", board, depth, maximizing, current_player, alpha, beta, transposition_table): #, time_counter):\n",
    "\n",
    "    #state = tuple(board.flatten())\n",
    "    state = board_to_state(board)\n",
    "    \n",
    "    #time_start = time.perf_counter()\n",
    "    if state in transposition_table: return transposition_table[state]\n",
    "    #time_counter.add_t((time.perf_counter() - time_start) * 1e6)\n",
    "\n",
    "    winner = game.check_winner()\n",
    "    if winner != -1 or depth == 0:\n",
    "        return evaluate_board(board, winner, current_player if maximizing else 1 - current_player)\n",
    "\n",
    "    if maximizing:\n",
    "        max_eval = float('-inf')\n",
    "        for om in ALL_MOVES:\n",
    "            from_pos, move = om\n",
    "            new_board, ok = game.single_move(board, from_pos, move, current_player)\n",
    "            if ok:\n",
    "                eval = minimax(game, new_board, depth - 1, False, 1 - current_player, alpha, beta, transposition_table) #, time_counter)\n",
    "\n",
    "                max_eval = max(max_eval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "\n",
    "                if beta <= alpha: break  # Prune the remaining branches\n",
    "\n",
    "        transposition_table[state] = max_eval\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for om in ALL_MOVES:\n",
    "            from_pos, move = om\n",
    "            new_board, ok = game.single_move(board, from_pos, move, current_player)\n",
    "            if ok:\n",
    "                eval = minimax(game, new_board, depth - 1, True, 1 - current_player, alpha, beta, transposition_table) #, time_counter)\n",
    "\n",
    "                min_eval = min(min_eval, eval)\n",
    "                beta = min(beta, eval)\n",
    "\n",
    "                if beta <= alpha: break  # Prune the remaining branches\n",
    "\n",
    "        transposition_table[state] = min_eval\n",
    "        return min_eval\n",
    "    \n",
    "def evaluate_board(board, winner, current_player):\n",
    "\n",
    "    bonus = 0\n",
    "    diag_a_player = 0\n",
    "    diag_a_enemy = 0\n",
    "    diag_b_player = 0\n",
    "    diag_b_enemy = 0\n",
    "    for i in range(5):\n",
    "        line = board[i, :]\n",
    "        bonus += pow(sum(line == current_player), 2) - pow(sum(line == 1 - current_player), 2)\n",
    "        line = board[:, i]\n",
    "        bonus += pow(sum(line == current_player), 2) - pow(sum(line == 1 - current_player), 2)\n",
    "        if board[i, i] == current_player: diag_a_player += 1\n",
    "        elif board[i, i] == 1 - current_player: diag_a_enemy += 1\n",
    "        if board[i, 4-i] == current_player: diag_b_player += 1\n",
    "        elif board[i, 4-i] == 1 - current_player: diag_b_enemy += 1\n",
    "\n",
    "    bonus += pow(diag_a_player, 2) - pow(diag_a_enemy, 2)\n",
    "    bonus += pow(diag_b_player, 2) - pow(diag_b_enemy, 2)\n",
    "\n",
    "    base_reward = -100 # winner == 1 - current_player\n",
    "    if winner == current_player: base_reward = 100\n",
    "    elif winner == -1: base_reward = 0\n",
    "\n",
    "    return base_reward + bonus\n",
    "\n",
    "def minimax_wrapper(board, player_id, max_depth= 2, transposition_table= {}, return_val= False):\n",
    "\n",
    "    game = Dummy_Game()\n",
    "\n",
    "    best_move = None\n",
    "    best_eval = float('-inf')\n",
    "\n",
    "    values = np.zeros(shape= (len(ALL_MOVES),))\n",
    "\n",
    "    #time_counter = TimeCounter()\n",
    "    \n",
    "    for i_m, m in enumerate(ALL_MOVES):\n",
    "        from_pos, move = m\n",
    "        new_board, ok = game.single_move(board, from_pos, move, player_id)\n",
    "        if ok:\n",
    "            om_eval = minimax(game, new_board, max_depth, False, 1 - player_id, float('-inf'), float('inf'), transposition_table) #, time_counter)\n",
    "\n",
    "            if om_eval > best_eval:\n",
    "                best_eval = om_eval\n",
    "                best_move = m\n",
    "\n",
    "            values[i_m] = om_eval\n",
    "\n",
    "    #print(f\"Best move: {best_move} -> Best Value: {best_eval}\")\n",
    "    \n",
    "    #tt, avgt = time_counter.get()\n",
    "    #print(f\"Elapsed time: {(tt, avgt)} microseconds\")\n",
    "    #print(f\"Elapsed time: {tt / 1e6, avgt / 1e6} seconds\")\n",
    "\n",
    "    if return_val: return values\n",
    "\n",
    "    return best_move\n",
    "\n",
    "initial_board = np.ones((5, 5)) * -1\n",
    "\n",
    "minimax_wrapper(initial_board, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:28:45<00:00,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "randP = RandomPlayer()\n",
    "\n",
    "n_trials = 1000\n",
    "\n",
    "moves_seen = {}\n",
    "\n",
    "for i_g in tqdm(range(n_trials)):\n",
    "\n",
    "    game = Dummy_Game()\n",
    "\n",
    "    while game.check_winner() == -1:\n",
    "\n",
    "        ## Dense turn\n",
    "\n",
    "        board = game.get_board()\n",
    "        player_id = game.current_player_idx\n",
    "\n",
    "        mini_values = minimax_wrapper(board, player_id, 1, {}, True)\n",
    "\n",
    "        from_pos, move = ALL_MOVES[np.argmax(mini_values)]\n",
    "\n",
    "        state = list(board.flatten())\n",
    "        state.append(player_id)\n",
    "        state = tuple(state)\n",
    "\n",
    "        if state not in moves_seen.keys(): moves_seen[state] = mini_values\n",
    "\n",
    "        ## Random turn\n",
    "\n",
    "        ok = False\n",
    "        while not ok:\n",
    "            from_pos, move = randP.make_move(game)\n",
    "            ok = game.do_move(from_pos, move, game.current_player_idx)\n",
    "\n",
    "print(len(moves_seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DenseMini(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(DenseMini, self).__init__()\n",
    "\n",
    "        self.input_shape = (3, 5, 5)\n",
    "        self.dense_input_shape = 75\n",
    "        self.action_size = N_ALL\n",
    "\n",
    "        # first\n",
    "\n",
    "        self.flatten = nn.Flatten(0)\n",
    "        self.dense1 = nn.Linear(self.dense_input_shape, self.action_size * 4)\n",
    "        self.dense2 = nn.Linear(self.action_size * 4, self.action_size * 2)\n",
    "        self.dense3 = nn.Linear(self.action_size * 2, self.action_size)\n",
    "\n",
    "        # second\n",
    "\n",
    "#        self.flatten = nn.Flatten(0)\n",
    "#        self.dense1 = nn.Linear(self.dense_input_shape, self.action_size * 40)\n",
    "#        self.dense2 = nn.Linear(self.action_size * 40, self.action_size * 20)\n",
    "#        self.dense3 = nn.Linear(self.action_size * 20, self.action_size)\n",
    "\n",
    "        # third\n",
    "\n",
    "#        self.flatten = nn.Flatten(0)\n",
    "#\n",
    "#        self.dense1 = nn.Linear(self.dense_input_shape, self.dense_input_shape * 2)\n",
    "#        self.dense2 = nn.Linear(self.dense_input_shape * 2, self.action_size) # output (44,)\n",
    "#\n",
    "#        self.conv1 = nn.Conv2d(3, self.action_size, 3, 1)\n",
    "#        self.conv11 = nn.Conv2d(self.action_size + 3, self.action_size, 2, 1)\n",
    "#        self.conv111 = nn.Conv2d(self.action_size * 2 + 3, self.action_size, 2, 1) # should output (44,)\n",
    "#\n",
    "#        self.conv2 = nn.Conv2d(3, self.action_size, 2, 1)\n",
    "#        self.conv22 = nn.Conv2d(self.action_size + 3, self.action_size, 2, 1)\n",
    "#        self.conv222 = nn.Conv2d(self.action_size * 2 + 3, self.action_size, 2, 1)\n",
    "#        self.conv2222 = nn.Conv2d(self.action_size * 3 + 3, self.action_size, 2, 1)  # should output (44,)\n",
    "#\n",
    "#        self.out_dense1 = nn.Linear(self.action_size * 3, self.action_size * 2)\n",
    "#        self.out_dense2 = nn.Linear(self.action_size * 2, self.action_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first and second\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        x = torch.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        # third\n",
    "#\n",
    "#        print('---------------------------------')\n",
    "#        print(f'x: {x.shape}')\n",
    "#        print('-')\n",
    "#\n",
    "#        x_flatten = self.flatten(x)\n",
    "#\n",
    "#        print(f'x_flatten: {x_flatten.shape}')\n",
    "#        print('-')\n",
    "#\n",
    "#        x0 = torch.relu(self.dense1(x_flatten))\n",
    "#        print(f'x0: {x0.shape}')\n",
    "#        x0 = torch.relu(self.dense2(x0))\n",
    "#\n",
    "#        print(f'x0: {x0.shape}')\n",
    "#        print('-')\n",
    "#\n",
    "#        x1 = torch.relu(self.conv1(x))\n",
    "#        print(f'x1: {x1.shape}')\n",
    "#        x1 = torch.cat([x1, x], dim= 0)\n",
    "#        print(f'x1: {x1.shape}')\n",
    "#        x11 = torch.relu(self.conv11(x1))\n",
    "#        print(f'x11: {x11.shape}')\n",
    "#        x11 = torch.cat([x11, x1, x], dim= 0)\n",
    "#        print(f'x11: {x11.shape}')\n",
    "#        x111 = torch.relu(self.conv111(x11))\n",
    "#        print(f'x111: {x111.shape}')\n",
    "#        x111 = self.flatten(x111)\n",
    "#\n",
    "#        print(f'x111: {x111.shape}')\n",
    "#        print('-')\n",
    "#\n",
    "#        x2 = torch.relu(self.conv2(x))\n",
    "#        print(f'x2: {x2.shape}')\n",
    "#        x2 = torch.cat([x2, x], dim= 0)\n",
    "#        print(f'x2: {x2.shape}')\n",
    "#        x22 = torch.relu(self.conv22(x2))\n",
    "#        print(f'x22: {x22.shape}')\n",
    "#        x22 = torch.cat([x22, x2, x], dim= 0)\n",
    "#        print(f'x22: {x22.shape}')\n",
    "#        x222 = torch.relu(self.conv222(x22))\n",
    "#        print(f'x222: {x222.shape}')\n",
    "#        x222 = torch.cat([x222, x22, x2, x], dim= 0)\n",
    "#        print(f'x222: {x222.shape}')\n",
    "#        x2222 = torch.relu(self.conv2222(x222))\n",
    "#        print(f'x2222: {x2222.shape}')\n",
    "#        x2222 = self.flatten(x2222)\n",
    "#\n",
    "#        print(f'x2222: {x2222.shape}')\n",
    "#        print('-')\n",
    "#\n",
    "#        xio = torch.cat([x_flatten, x0, x111, x2222], dim= 0)\n",
    "#        print(f'xio: {xio.shape}')\n",
    "#        xio = torch.relu(self.out_dense1(xio))\n",
    "#        print(f'xio: {x.shape}')\n",
    "#        xio = self.out_dense2(xio)\n",
    "#\n",
    "#        print(f'xio: {xio.shape}')\n",
    "#        print('==============')\n",
    "#        \n",
    "#        x = xio\n",
    "\n",
    "        return x\n",
    "\n",
    "    def expand_board(self, x):\n",
    "\n",
    "        if self.player_id == 1:\n",
    "            new_x = np.ones(shape= x.shape) * -1\n",
    "            new_x[x == 0] = 1\n",
    "            new_x[x == 1] = 0\n",
    "            x = new_x\n",
    "\n",
    "        new_x = np.zeros(shape= (3, 5, 5))\n",
    "        new_x[0, x == -1] = 1\n",
    "        new_x[1, x == 0] = 1\n",
    "        new_x[2, x == 1] = 1\n",
    "\n",
    "        return torch.Tensor(new_x)\n",
    "    \n",
    "    def use(self, board, player_id):\n",
    "        self.player_id = player_id\n",
    "        x = self.expand_board(board)\n",
    "        return self.forward(x)\n",
    "\n",
    "class DensePlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = DenseMini()\n",
    "        self.rand = RandomPlayer()\n",
    "\n",
    "        self.last_move = None, None, None\n",
    "        self.until_draw = 10\n",
    "\n",
    "    def getDense(self): return self.dense\n",
    "    \n",
    "    def train(self, game: 'Game'):\n",
    "        return self.dense.use(game.get_board(), game.current_player_idx)\n",
    "    \n",
    "    def train_2(self, board, player_id):\n",
    "        return self.dense.use(board, player_id)\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        board = game.get_board()\n",
    "        player_id = game.current_player_idx\n",
    "        from_pos, move = ALL_MOVES[torch.argmax(self.dense.use(board, player_id).detach())]\n",
    "        \n",
    "        while board[from_pos] == 1 - player_id: from_pos, move = self.rand.make_move(game)\n",
    "\n",
    "        state = tuple(board.flatten())\n",
    "        if state == self.last_move[0] and from_pos == self.last_move[1] and move == self.last_move[2]:\n",
    "            self.until_draw -= 1\n",
    "            if self.until_draw == 0:\n",
    "                self.until_draw = 10\n",
    "                from_pos, move = RandomPlayer().make_move(game)\n",
    "            \n",
    "        self.last_move = (state, from_pos, move)\n",
    "\n",
    "        return from_pos, move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "avg_loss: 119225.15674942922\n",
      "-----------------------------------------------\n",
      "epoch: 1\n",
      "avg_loss: 74713.83855058505\n",
      "-----------------------------------------------\n",
      "epoch: 2\n",
      "avg_loss: 65197.14081228596\n",
      "-----------------------------------------------\n",
      "epoch: 3\n",
      "avg_loss: 61576.07533354737\n",
      "-----------------------------------------------\n",
      "epoch: 4\n",
      "avg_loss: 59238.26076448345\n",
      "-----------------------------------------------\n",
      "epoch: 5\n",
      "avg_loss: 57503.22281678082\n",
      "-----------------------------------------------\n",
      "epoch: 6\n",
      "avg_loss: 55954.52316103025\n",
      "-----------------------------------------------\n",
      "epoch: 7\n",
      "avg_loss: 54397.23071846461\n",
      "-----------------------------------------------\n",
      "epoch: 8\n",
      "avg_loss: 53057.526335973176\n",
      "-----------------------------------------------\n",
      "epoch: 9\n",
      "avg_loss: 51443.4044663242\n",
      "-----------------------------------------------\n",
      "epoch: 10\n",
      "avg_loss: 49950.98180650685\n",
      "-----------------------------------------------\n",
      "epoch: 11\n",
      "avg_loss: 48476.75018728596\n",
      "-----------------------------------------------\n",
      "epoch: 12\n",
      "avg_loss: 47543.94389447774\n",
      "-----------------------------------------------\n",
      "epoch: 13\n",
      "avg_loss: 46348.21131653111\n",
      "-----------------------------------------------\n",
      "epoch: 14\n",
      "avg_loss: 45529.8916862871\n",
      "-----------------------------------------------\n",
      "epoch: 15\n",
      "avg_loss: 44739.2155661387\n",
      "-----------------------------------------------\n",
      "epoch: 16\n",
      "avg_loss: 44034.09074450628\n",
      "-----------------------------------------------\n",
      "epoch: 17\n",
      "avg_loss: 43435.41872681221\n",
      "-----------------------------------------------\n",
      "epoch: 18\n",
      "avg_loss: 42922.31365938927\n",
      "-----------------------------------------------\n",
      "epoch: 19\n",
      "avg_loss: 42303.3433665097\n",
      "-----------------------------------------------\n",
      "epoch: 20\n",
      "avg_loss: 41742.27434717466\n",
      "-----------------------------------------------\n",
      "epoch: 21\n",
      "avg_loss: 41240.385367615585\n",
      "-----------------------------------------------\n",
      "epoch: 22\n",
      "avg_loss: 40801.9227668379\n",
      "-----------------------------------------------\n",
      "epoch: 23\n",
      "avg_loss: 40318.11951073773\n",
      "-----------------------------------------------\n",
      "epoch: 24\n",
      "avg_loss: 39941.30680115582\n",
      "-----------------------------------------------\n",
      "epoch: 25\n",
      "avg_loss: 39421.934173444635\n",
      "-----------------------------------------------\n",
      "epoch: 26\n",
      "avg_loss: 39007.03677939498\n",
      "-----------------------------------------------\n",
      "epoch: 27\n",
      "avg_loss: 38765.7244845177\n",
      "-----------------------------------------------\n",
      "epoch: 28\n",
      "avg_loss: 38481.05160620006\n",
      "-----------------------------------------------\n",
      "epoch: 29\n",
      "avg_loss: 38132.32906588898\n",
      "-----------------------------------------------\n",
      "epoch: 30\n",
      "avg_loss: 37872.98370612157\n",
      "-----------------------------------------------\n",
      "epoch: 31\n",
      "avg_loss: 37636.3921946347\n",
      "-----------------------------------------------\n",
      "epoch: 32\n",
      "avg_loss: 37369.504864975745\n",
      "-----------------------------------------------\n",
      "epoch: 33\n",
      "avg_loss: 37170.97111337043\n",
      "-----------------------------------------------\n",
      "epoch: 34\n",
      "avg_loss: 36959.17121949914\n",
      "-----------------------------------------------\n",
      "epoch: 35\n",
      "avg_loss: 36795.21030875428\n",
      "-----------------------------------------------\n",
      "epoch: 36\n",
      "avg_loss: 36587.332713505995\n",
      "-----------------------------------------------\n",
      "epoch: 37\n",
      "avg_loss: 36482.38923819207\n",
      "-----------------------------------------------\n",
      "epoch: 38\n",
      "avg_loss: 36285.93086918522\n",
      "-----------------------------------------------\n",
      "epoch: 39\n",
      "avg_loss: 36178.14746539669\n",
      "-----------------------------------------------\n",
      "epoch: 40\n",
      "avg_loss: 36021.3463809218\n",
      "-----------------------------------------------\n",
      "epoch: 41\n",
      "avg_loss: 35907.00155179795\n",
      "-----------------------------------------------\n",
      "epoch: 42\n",
      "avg_loss: 35725.41218518122\n",
      "-----------------------------------------------\n",
      "epoch: 43\n",
      "avg_loss: 35606.94232930223\n",
      "-----------------------------------------------\n",
      "epoch: 44\n",
      "avg_loss: 35438.555695276824\n",
      "-----------------------------------------------\n",
      "epoch: 45\n",
      "avg_loss: 35285.298859339324\n",
      "-----------------------------------------------\n",
      "epoch: 46\n",
      "avg_loss: 35109.75351384132\n",
      "-----------------------------------------------\n",
      "epoch: 47\n",
      "avg_loss: 35000.97759703196\n",
      "-----------------------------------------------\n",
      "epoch: 48\n",
      "avg_loss: 34896.14965485873\n",
      "-----------------------------------------------\n",
      "epoch: 49\n",
      "avg_loss: 34763.51196846461\n",
      "-----------------------------------------------\n",
      "epoch: 50\n",
      "avg_loss: 34679.79028877711\n",
      "-----------------------------------------------\n",
      "epoch: 51\n",
      "avg_loss: 34555.24491206478\n",
      "-----------------------------------------------\n",
      "epoch: 52\n",
      "avg_loss: 34439.014679651824\n",
      "-----------------------------------------------\n",
      "epoch: 53\n",
      "avg_loss: 34247.96728292666\n",
      "-----------------------------------------------\n",
      "epoch: 54\n",
      "avg_loss: 34209.75876230736\n",
      "-----------------------------------------------\n",
      "epoch: 55\n",
      "avg_loss: 34072.171099101026\n",
      "-----------------------------------------------\n",
      "epoch: 56\n",
      "avg_loss: 33979.03054990725\n",
      "-----------------------------------------------\n",
      "epoch: 57\n",
      "avg_loss: 33893.51670412386\n",
      "-----------------------------------------------\n",
      "epoch: 58\n",
      "avg_loss: 33726.419261914954\n",
      "-----------------------------------------------\n",
      "epoch: 59\n",
      "avg_loss: 33671.78976259275\n",
      "-----------------------------------------------\n",
      "epoch: 60\n",
      "avg_loss: 33504.110289133845\n",
      "-----------------------------------------------\n",
      "epoch: 61\n",
      "avg_loss: 33439.93396386273\n",
      "-----------------------------------------------\n",
      "epoch: 62\n",
      "avg_loss: 33383.76909870862\n",
      "-----------------------------------------------\n",
      "epoch: 63\n",
      "avg_loss: 33222.28420644264\n",
      "-----------------------------------------------\n",
      "epoch: 64\n",
      "avg_loss: 33224.093585009985\n",
      "-----------------------------------------------\n",
      "epoch: 65\n",
      "avg_loss: 33098.48077643407\n",
      "-----------------------------------------------\n",
      "epoch: 66\n",
      "avg_loss: 33079.64481663813\n",
      "-----------------------------------------------\n",
      "epoch: 67\n",
      "avg_loss: 32929.10248555223\n",
      "-----------------------------------------------\n",
      "epoch: 68\n",
      "avg_loss: 32946.52991224315\n",
      "-----------------------------------------------\n",
      "epoch: 69\n",
      "avg_loss: 32840.88191620291\n",
      "-----------------------------------------------\n",
      "epoch: 70\n",
      "avg_loss: 32793.979349493435\n",
      "-----------------------------------------------\n",
      "epoch: 71\n",
      "avg_loss: 32764.431074307933\n",
      "-----------------------------------------------\n",
      "epoch: 72\n",
      "avg_loss: 32676.672267408674\n",
      "-----------------------------------------------\n",
      "epoch: 73\n",
      "avg_loss: 32697.460991010274\n",
      "-----------------------------------------------\n",
      "epoch: 74\n",
      "avg_loss: 32607.51864833048\n",
      "-----------------------------------------------\n",
      "epoch: 75\n",
      "avg_loss: 32482.24883169235\n",
      "-----------------------------------------------\n",
      "epoch: 76\n",
      "avg_loss: 32541.325404894407\n",
      "-----------------------------------------------\n",
      "epoch: 77\n",
      "avg_loss: 32466.454859624715\n",
      "-----------------------------------------------\n",
      "epoch: 78\n",
      "avg_loss: 32346.794364476315\n",
      "-----------------------------------------------\n",
      "epoch: 79\n",
      "avg_loss: 32310.064065175513\n",
      "-----------------------------------------------\n",
      "epoch: 80\n",
      "avg_loss: 32282.60333279823\n",
      "-----------------------------------------------\n",
      "epoch: 81\n",
      "avg_loss: 32240.356436394122\n",
      "-----------------------------------------------\n",
      "epoch: 82\n",
      "avg_loss: 32200.612790739156\n",
      "-----------------------------------------------\n",
      "epoch: 83\n",
      "avg_loss: 32042.969084439213\n",
      "-----------------------------------------------\n",
      "epoch: 84\n",
      "avg_loss: 32111.26223601598\n",
      "-----------------------------------------------\n",
      "epoch: 85\n",
      "avg_loss: 32032.80131189355\n",
      "-----------------------------------------------\n",
      "epoch: 86\n",
      "avg_loss: 32010.940728453195\n",
      "-----------------------------------------------\n",
      "epoch: 87\n",
      "avg_loss: 32011.156285673515\n",
      "-----------------------------------------------\n",
      "epoch: 88\n",
      "avg_loss: 31981.148040632135\n",
      "-----------------------------------------------\n",
      "epoch: 89\n",
      "avg_loss: 31870.3133918379\n",
      "-----------------------------------------------\n",
      "epoch: 90\n",
      "avg_loss: 31836.798966359875\n",
      "-----------------------------------------------\n",
      "epoch: 91\n",
      "avg_loss: 31881.785504066782\n",
      "-----------------------------------------------\n",
      "epoch: 92\n",
      "avg_loss: 31795.788763734305\n",
      "-----------------------------------------------\n",
      "epoch: 93\n",
      "avg_loss: 31782.257861551083\n",
      "-----------------------------------------------\n",
      "epoch: 94\n",
      "avg_loss: 31829.13318707192\n",
      "-----------------------------------------------\n",
      "epoch: 95\n",
      "avg_loss: 31803.544854987158\n",
      "-----------------------------------------------\n",
      "epoch: 96\n",
      "avg_loss: 31698.021685038526\n",
      "-----------------------------------------------\n",
      "epoch: 97\n",
      "avg_loss: 31684.762673016554\n",
      "-----------------------------------------------\n",
      "epoch: 98\n",
      "avg_loss: 31674.568970283963\n",
      "-----------------------------------------------\n",
      "epoch: 99\n",
      "avg_loss: 31663.292460402397\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = DensePlayer()\n",
    "\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(agent.getDense().parameters(), lr= LEARNING_RATE)\n",
    "\n",
    "ms_list = list(moves_seen.items())\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    avg_loss = 0\n",
    "    n_loss = 0\n",
    "    cumulative_loss = None\n",
    "\n",
    "    idx_rand = np.argsort(np.random.random(size= (len(ms_list,))))\n",
    "    ms_list = [ms_list[i] for i in idx_rand]\n",
    "\n",
    "    for i_m in range(len(ms_list)):\n",
    "\n",
    "        state, mini_values = ms_list[i_m]\n",
    "        board = np.array(state[:-1], dtype=np.uint8).reshape(5, 5)\n",
    "        pid = state[-1]\n",
    "\n",
    "        #board, pid, mini_values = moves_seen[i_m]\n",
    "\n",
    "        agent_values = agent.train_2(board, pid)\n",
    "\n",
    "        if cumulative_loss is None: cumulative_loss = criterion(torch.Tensor(mini_values), agent_values)\n",
    "        else: cumulative_loss += criterion(torch.Tensor(mini_values), agent_values)\n",
    "\n",
    "        if i_m % BATCH_SIZE == BATCH_SIZE - 1:\n",
    "            optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += cumulative_loss.item()\n",
    "            cumulative_loss = None\n",
    "            n_loss += 1\n",
    "\n",
    "    print(f'avg_loss: {avg_loss / n_loss}')\n",
    "    print('-----------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "==============================================================================\n",
      "game 1 - Player First (0)\n",
      "[[ 0  0 -1 -1  0]\n",
      " [ 0  1  0 -1 -1]\n",
      " [ 0 -1 -1 -1  1]\n",
      " [ 0 -1  1 -1 -1]\n",
      " [ 0  0  1  1  1]]\n",
      "Player win\n",
      "game 1 - Player Second (1)\n",
      "[[ 0  1  1  0  0]\n",
      " [ 1  1  1  1  1]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0 -1 -1 -1 -1]\n",
      " [ 0  0  1  0  0]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 2 - Player First (0)\n",
      "[[ 0 -1 -1  0  1]\n",
      " [ 0 -1 -1 -1 -1]\n",
      " [ 0 -1 -1 -1  1]\n",
      " [ 0 -1 -1 -1 -1]\n",
      " [ 0 -1  1  1  1]]\n",
      "Player win\n",
      "game 2 - Player Second (1)\n",
      "[[-1 -1 -1  0  1]\n",
      " [-1 -1 -1 -1  1]\n",
      " [-1 -1  0 -1  1]\n",
      " [ 0  1  0 -1  1]\n",
      " [ 0  0 -1  0  1]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 3 - Player First (0)\n",
      "[[ 0  1  0  1  1]\n",
      " [ 1  0 -1 -1  1]\n",
      " [ 1 -1  0 -1 -1]\n",
      " [ 0 -1 -1  0  1]\n",
      " [ 1 -1 -1 -1  0]]\n",
      "Player win\n",
      "game 3 - Player Second (1)\n",
      "[[ 0  0  0  1  0]\n",
      " [ 0  1  0  0  1]\n",
      " [ 1  1  0 -1  1]\n",
      " [ 1  0  1  0  1]\n",
      " [ 0  1  1 -1 -1]]\n",
      "Player lose\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 4 - Player First (0)\n",
      "[[ 0  0  0  0  0]\n",
      " [ 1  0 -1 -1  1]\n",
      " [ 1 -1 -1 -1  1]\n",
      " [ 0  1 -1 -1 -1]\n",
      " [ 0  0  1  1 -1]]\n",
      "Player win\n",
      "game 4 - Player Second (1)\n",
      "[[ 0 -1 -1  1  1]\n",
      " [ 1 -1 -1  1  1]\n",
      " [-1 -1  1  0  1]\n",
      " [ 1  0  1  0  1]\n",
      " [ 0  0  0  0  1]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 5 - Player First (0)\n",
      "[[ 0  0  0  0  0]\n",
      " [ 1  1  0 -1  0]\n",
      " [ 1 -1 -1 -1  0]\n",
      " [ 1  0 -1 -1  1]\n",
      " [ 1 -1  1  0  0]]\n",
      "Player win\n",
      "game 5 - Player Second (1)\n",
      "[[ 1  0  0  1  0]\n",
      " [ 1 -1  0 -1  0]\n",
      " [ 1 -1  1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1  0 -1  1  0]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 6 - Player First (0)\n",
      "[[ 0  0  0  1  1]\n",
      " [ 1 -1  1 -1  1]\n",
      " [ 0  0  0  1  1]\n",
      " [ 0  0 -1 -1  1]\n",
      " [ 1  0  1  1  1]]\n",
      "Player lose\n",
      "game 6 - Player Second (1)\n",
      "[[ 1 -1  1  0  1]\n",
      " [ 0  1 -1 -1  1]\n",
      " [ 0  0 -1 -1  1]\n",
      " [-1 -1 -1 -1  1]\n",
      " [ 0  0 -1  0  1]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 7 - Player First (0)\n",
      "[[ 0 -1  0 -1  1]\n",
      " [ 0  1 -1 -1 -1]\n",
      " [ 0 -1 -1 -1  1]\n",
      " [ 0 -1 -1  1  0]\n",
      " [ 0 -1  1  0  1]]\n",
      "Player win\n",
      "game 7 - Player Second (1)\n",
      "[[ 1  1  1  1  1]\n",
      " [ 1 -1 -1 -1  0]\n",
      " [ 0 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 0 -1  0  1  0]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 8 - Player First (0)\n",
      "[[ 0  1  1  1  1]\n",
      " [ 0 -1  1  0  0]\n",
      " [ 0  1  1  1 -1]\n",
      " [ 0  1 -1 -1  0]\n",
      " [ 0  0  1 -1  1]]\n",
      "Player win\n",
      "game 8 - Player Second (1)\n",
      "[[ 0  0  1 -1  0]\n",
      " [ 1 -1 -1  0  1]\n",
      " [ 1  1 -1  1  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 1  1  1  1  1]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 9 - Player First (0)\n",
      "[[ 1  0  1 -1  1]\n",
      " [-1  1 -1 -1  1]\n",
      " [-1 -1 -1 -1  0]\n",
      " [ 0 -1  1  0  1]\n",
      " [ 0  0  0  0  0]]\n",
      "Player win\n",
      "game 9 - Player Second (1)\n",
      "[[ 0  1  1  1  0]\n",
      " [ 1  1 -1 -1  0]\n",
      " [ 0  0 -1 -1  1]\n",
      " [ 0 -1 -1  1  0]\n",
      " [ 1  1  1  1  1]]\n",
      "Player win\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "game 10 - Player First (0)\n",
      "[[ 1 -1  1 -1  1]\n",
      " [-1 -1  0  0  1]\n",
      " [ 0 -1  0 -1  1]\n",
      " [-1  0 -1  1  1]\n",
      " [ 0  0 -1  0  1]]\n",
      "Player lose\n",
      "game 10 - Player Second (1)\n",
      "[[ 1  0  0 -1  0]\n",
      " [ 1  1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1  0 -1 -1  1]]\n",
      "Player win\n",
      "Player won 8 / 10 as first\n",
      "Player won 9 / 10 as second\n"
     ]
    }
   ],
   "source": [
    "ThePlayer = agent\n",
    "\n",
    "wins_first = 0\n",
    "wins_second = 0\n",
    "n_trials = 10\n",
    "\n",
    "for i_g in range(n_trials):\n",
    "\n",
    "    print('==============================================================================')\n",
    "    print('==============================================================================')\n",
    "    print(f'game {i_g+1} - Player First (0)')\n",
    "\n",
    "    game = Game()\n",
    "    winner = game.play(ThePlayer, RandomPlayer())\n",
    "    if winner == 0:\n",
    "        print(game.get_board())\n",
    "        print('Player win')\n",
    "        wins_first += 1\n",
    "    else:\n",
    "        print(game.get_board())\n",
    "        print('Player lose')\n",
    "\n",
    "    print(f'game {i_g+1} - Player Second (1)')\n",
    "\n",
    "    game = Game()\n",
    "    winner = game.play(RandomPlayer(), ThePlayer)\n",
    "    if winner == 1:\n",
    "        print(game.get_board())\n",
    "        print('Player win')\n",
    "        wins_second += 1\n",
    "    else:\n",
    "        print(game.get_board())\n",
    "        print('Player lose')\n",
    "\n",
    "print(f\"Player won {wins_first} / {n_trials} as first\")\n",
    "print(f\"Player won {wins_second} / {n_trials} as second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
